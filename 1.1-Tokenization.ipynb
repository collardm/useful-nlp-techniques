{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4\n"
     ]
    }
   ],
   "source": [
    "# import nltk and check on the version\n",
    "\n",
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The early bird gets the worm. Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb. The Earth is 92,960,000mi from the Sun. In Mr. Smith's words, 'This book is great!' The cost is $19.99\\non sale until the end of the year. Michio Kaku, Ph.D. \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a block of text to represent a corpus\n",
    "\n",
    "text = \"The early bird gets the worm. \" + \\\n",
    "       \"Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb. \" + \\\n",
    "       \"The Earth is 92,960,000mi from the Sun. \" + \\\n",
    "       \"In Mr. Smith's words, 'This book is great!' \" + \\\n",
    "       \"The cost is $19.99\\non sale until the end of the year. \" + \\\n",
    "       \"Michio Kaku, Ph.D. \"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\maxen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download and install nltk components needed for sentence tokenization (punkt)\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The early bird gets the worm.',\n",
       " 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb.',\n",
       " 'The Earth is 92,960,000mi from the Sun.',\n",
       " \"In Mr. Smith's words, 'This book is great!'\",\n",
       " 'The cost is $19.99\\non sale until the end of the year.',\n",
       " 'Michio Kaku, Ph.D.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the text to sentence tokens\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(text)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The early bird gets the worm.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can access each sentence as an element of the list\n",
    "\n",
    "sent_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'early', 'bird', 'gets', 'the', 'worm', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenize the first sentence\n",
    "\n",
    "nltk.word_tokenize(sent_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'early', 'bird', 'gets', 'the', 'worm', '.'],\n",
       " ['Dr.',\n",
       "  'Strangelove',\n",
       "  'or',\n",
       "  ':',\n",
       "  'How',\n",
       "  'I',\n",
       "  'Learned',\n",
       "  'to',\n",
       "  'Stop',\n",
       "  'Worrying',\n",
       "  'and',\n",
       "  'Love',\n",
       "  'the',\n",
       "  'Bomb',\n",
       "  '.'],\n",
       " ['The', 'Earth', 'is', '92,960,000mi', 'from', 'the', 'Sun', '.'],\n",
       " ['In',\n",
       "  'Mr.',\n",
       "  'Smith',\n",
       "  \"'s\",\n",
       "  'words',\n",
       "  ',',\n",
       "  \"'This\",\n",
       "  'book',\n",
       "  'is',\n",
       "  'great',\n",
       "  '!',\n",
       "  \"'\"],\n",
       " ['The',\n",
       "  'cost',\n",
       "  'is',\n",
       "  '$',\n",
       "  '19.99',\n",
       "  'on',\n",
       "  'sale',\n",
       "  'until',\n",
       "  'the',\n",
       "  'end',\n",
       "  'of',\n",
       "  'the',\n",
       "  'year',\n",
       "  '.'],\n",
       " ['Michio', 'Kaku', ',', 'Ph.D', '.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize all sentences.  Notice the list of lists\n",
    "\n",
    "[nltk.word_tokenize(sent_token) for sent_token in sent_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'early',\n",
       " 'bird',\n",
       " 'gets',\n",
       " 'the',\n",
       " 'worm',\n",
       " '.',\n",
       " 'Dr.',\n",
       " 'Strangelove',\n",
       " 'or',\n",
       " ':',\n",
       " 'How',\n",
       " 'I',\n",
       " 'Learned',\n",
       " 'to',\n",
       " 'Stop',\n",
       " 'Worrying',\n",
       " 'and',\n",
       " 'Love',\n",
       " 'the',\n",
       " 'Bomb',\n",
       " '.',\n",
       " 'The',\n",
       " 'Earth',\n",
       " 'is',\n",
       " '92,960,000mi',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Sun',\n",
       " '.',\n",
       " 'In',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " \"'s\",\n",
       " 'words',\n",
       " ',',\n",
       " \"'This\",\n",
       " 'book',\n",
       " 'is',\n",
       " 'great',\n",
       " '!',\n",
       " \"'\",\n",
       " 'The',\n",
       " 'cost',\n",
       " 'is',\n",
       " '$',\n",
       " '19.99',\n",
       " 'on',\n",
       " 'sale',\n",
       " 'until',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'the',\n",
       " 'year',\n",
       " '.',\n",
       " 'Michio',\n",
       " 'Kaku',\n",
       " ',',\n",
       " 'Ph.D',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also word tokenize the entire document\n",
    "# notice this results in a single list\n",
    "\n",
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explicitly create a Punkt sentence tokenizer\n",
    "ps_tokenizer = nltk.PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The early bird gets the worm.',\n",
       " 'Dr.',\n",
       " 'Strangelove or: How I Learned to Stop Worrying and Love the Bomb.',\n",
       " 'The Earth is 92,960,000mi from the Sun.',\n",
       " 'In Mr.',\n",
       " \"Smith's words, 'This book is great!'\",\n",
       " 'The cost is $19.99\\non sale until the end of the year.',\n",
       " 'Michio Kaku, Ph.D.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize sentences\n",
    "\n",
    "ps_tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.', 'Strangelove or: How I Learned to Stop Worrying and Love the Bomb.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that Punkt treats Dr. as a sifferent token / sentence\n",
    "\n",
    "ps_tokenizer.tokenize(sent_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# versus the default which does not\n",
    "\n",
    "nltk.sent_tokenize(sent_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenization can be done explicitly with a Treebank Word Tokenizer\n",
    "\n",
    "tw_tokenizer = nltk.TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'Strangelove',\n",
       " 'or',\n",
       " ':',\n",
       " 'How',\n",
       " 'I',\n",
       " 'Learned',\n",
       " 'to',\n",
       " 'Stop',\n",
       " 'Worrying',\n",
       " 'and',\n",
       " 'Love',\n",
       " 'the',\n",
       " 'Bomb',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word tokenize the second sentence\n",
    "\n",
    "tw_tokenizer.tokenize(sent_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'Strangelove',\n",
       " 'or',\n",
       " ':',\n",
       " 'How',\n",
       " 'I',\n",
       " 'Learned',\n",
       " 'to',\n",
       " 'Stop',\n",
       " 'Worrying',\n",
       " 'and',\n",
       " 'Love',\n",
       " 'the',\n",
       " 'Bomb',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that this is the same result as the default\n",
    "\n",
    "nltk.word_tokenize(sent_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Earth', 'is', '92', '960', '000mi', 'from', 'the', 'Sun']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way to tokenize words is with the Regexp tokenizer\n",
    "\n",
    "regex_pattern = r'\\w+' # word characters\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=regex_pattern, gaps=False)\n",
    "regex_wt.tokenize(sent_tokens[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Earth', 'is', '92,960,000mi', 'from', 'the', 'Sun.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or split it on whitespace characters\n",
    "\n",
    "regex_pattern = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=regex_pattern, gaps=True)\n",
    "regex_wt.tokenize(sent_tokens[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'Mr.', \"Smith's\", 'words,', \"'This\", 'book', 'is', \"great!'\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and there is a tokenizer to explicitly s split on white space\n",
    "\n",
    "ws_wt = nltk.WhitespaceTokenizer()\n",
    "ws_wt.tokenize(sent_tokens[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "czech.pickle\ndanish.pickle\ndutch.pickle\nenglish.pickle\nestonian.pickle\nfinnish.pickle\nfrench.pickle\ngerman.pickle\ngreek.pickle\nitalian.pickle\nnorwegian.pickle\npolish.pickle\nportuguese.pickle\nPY3\nREADME\nrussian.pickle\nslovene.pickle\nspanish.pickle\nswedish.pickle\nturkish.pickle\n"
    }
   ],
   "source": [
    "# Punkt has support for several languages.  There are each stored in a pickle file\n",
    "\n",
    "!dir /B C:\\Users\\maxen\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cosi` l'animo mio, ch'ancor fuggiva,  si volse a retro a rimirar lo passo che non lascio` gia` mai persona viva. Poi ch'ei posato un poco il corpo lasso, ripresi via per la piaggia diserta, si` che 'l pie` fermo sempre era 'l piu` basso.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's tokenize some itelian (from Dante's Inferno)\n",
    "\n",
    "dante = \"cosi` l'animo mio, ch'ancor fuggiva,  si volse a retro a rimirar lo passo che non lascio` \" + \\\n",
    "        \"gia` mai persona viva. Poi ch'ei posato un poco il corpo lasso, ripresi via per la piaggia diserta, \" + \\\n",
    "        \"si` che 'l pie` fermo sempre era 'l piu` basso.\"\n",
    "dante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"cosi` l'animo mio, ch'ancor fuggiva,  si volse a retro a rimirar lo passo che non lascio` gia` mai persona viva.\",\n",
       " \"Poi ch'ei posato un poco il corpo lasso, ripresi via per la piaggia diserta, si` che 'l pie` fermo sempre era 'l piu` basso.\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the default always tokenizes with English\n",
    "\n",
    "nltk.sent_tokenize(dante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"cosi` l'animo mio, ch'ancor fuggiva,  si volse a retro a rimirar lo passo che non lascio` gia` mai persona viva.\",\n",
       " \"Poi ch'ei posato un poco il corpo lasso, ripresi via per la piaggia diserta, si` che 'l pie` fermo sempre era 'l piu` basso.\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but we can use other languages using the languag4e parameter and the language name\n",
    "\n",
    "nltk.sent_tokenize(dante, language=\"italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"cosi` l'animo mio, ch'ancor fuggiva,  si volse a retro a rimirar lo passo che non lascio` gia` mai persona viva.\",\n",
       " \"Poi ch'ei posato un poco il corpo lasso, ripresi via per la piaggia diserta, si` che 'l pie` fermo sempre era 'l piu` basso.\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we can also can create a tokenizer directly from the pickle\n",
    "italian_tokenizer = nltk.data.load(\"tokenizers/punkt/italian.pickle\")\n",
    "italian_tokenizer.tokenize(dante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cosi',\n",
       " '`',\n",
       " \"l'animo\",\n",
       " 'mio',\n",
       " ',',\n",
       " \"ch'ancor\",\n",
       " 'fuggiva',\n",
       " ',',\n",
       " 'si',\n",
       " 'volse',\n",
       " 'a',\n",
       " 'retro',\n",
       " 'a',\n",
       " 'rimirar',\n",
       " 'lo',\n",
       " 'passo',\n",
       " 'che',\n",
       " 'non',\n",
       " 'lascio',\n",
       " '`',\n",
       " 'gia',\n",
       " '`',\n",
       " 'mai',\n",
       " 'persona',\n",
       " 'viva',\n",
       " '.',\n",
       " 'Poi',\n",
       " \"ch'ei\",\n",
       " 'posato',\n",
       " 'un',\n",
       " 'poco',\n",
       " 'il',\n",
       " 'corpo',\n",
       " 'lasso',\n",
       " ',',\n",
       " 'ripresi',\n",
       " 'via',\n",
       " 'per',\n",
       " 'la',\n",
       " 'piaggia',\n",
       " 'diserta',\n",
       " ',',\n",
       " 'si',\n",
       " '`',\n",
       " 'che',\n",
       " \"'\",\n",
       " 'l',\n",
       " 'pie',\n",
       " '`',\n",
       " 'fermo',\n",
       " 'sempre',\n",
       " 'era',\n",
       " \"'\",\n",
       " 'l',\n",
       " 'piu',\n",
       " '`',\n",
       " 'basso',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is also supported in the word tokenizer\n",
    "\n",
    "nltk.word_tokenize(dante, language=\"italian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " \"'s\",\n",
       " 'words',\n",
       " ',',\n",
       " \"'This\",\n",
       " 'book',\n",
       " 'is',\n",
       " 'great',\n",
       " '!',\n",
       " \"'\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll work with sentence 4, so let's check it out\n",
    "\n",
    "nltk.word_tokenize(sent_tokens[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'mr.',\n",
       " 'smith',\n",
       " \"'s\",\n",
       " 'words',\n",
       " ',',\n",
       " \"'this\",\n",
       " 'book',\n",
       " 'is',\n",
       " 'great',\n",
       " '!',\n",
       " \"'\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a function to lower all tokens in a list\n",
    "\n",
    "def lower_tokens(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "# lower all the tokens in our list\n",
    "lower_tokens(nltk.word_tokenize(sent_tokens[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this contains a list of all punctuation\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a function to remove all punctuation in each token in a list\n",
    "\n",
    "def remove_punctuation_tokens(tokens):\n",
    "    punct_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    return [a for a,b in zip(tokens, [punct_regex.sub('', token) for token in tokens]) if b != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'mr.', 'smith', \"'s\", 'words', \"'this\", 'book', 'is', 'great']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and give it a try\n",
    "\n",
    "remove_punctuation_tokens(lower_tokens(nltk.word_tokenize(sent_tokens[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and let's create a function that lowers and removes punctuation\n",
    "\n",
    "def get_cleaned_tokens(tokens):\n",
    "    return remove_punctuation_tokens(lower_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'mr.', 'smith', \"'s\", 'words', \"'this\", 'book', 'is', 'great']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cleaned_tokens(nltk.word_tokenize(sent_tokens[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}