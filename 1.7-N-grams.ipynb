{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'New York City .NET Core Microsoft Azure Natural Language Processing'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a string to demonstrate n-gram identification\n",
    "\n",
    "text = \"New York City \" + \\\n",
    "       \".NET Core \" + \\\n",
    "       \"Microsoft Azure \" + \\\n",
    "       \"Natural Language Processing\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['New',\n 'York',\n 'City',\n '.NET',\n 'Core',\n 'Microsoft',\n 'Azure',\n 'Natural',\n 'Language',\n 'Processing']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('New', 'York'),\n ('York', 'City'),\n ('City', '.NET'),\n ('.NET', 'Core'),\n ('Core', 'Microsoft'),\n ('Microsoft', 'Azure'),\n ('Azure', 'Natural'),\n ('Natural', 'Language'),\n ('Language', 'Processing')]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find bi-grams (2-grams)\n",
    "\n",
    "bigrams = nltk.bigrams(tokens)\n",
    "list(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('New', 'York', 'City'),\n ('York', 'City', '.NET'),\n ('City', '.NET', 'Core'),\n ('.NET', 'Core', 'Microsoft'),\n ('Core', 'Microsoft', 'Azure'),\n ('Microsoft', 'Azure', 'Natural'),\n ('Azure', 'Natural', 'Language'),\n ('Natural', 'Language', 'Processing')]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find tri-grams (3-grams)\n",
    "\n",
    "trigrams = nltk.trigrams(tokens)\n",
    "list(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('New', 'York', 'City', '.NET'),\n ('York', 'City', '.NET', 'Core'),\n ('City', '.NET', 'Core', 'Microsoft'),\n ('.NET', 'Core', 'Microsoft', 'Azure'),\n ('Core', 'Microsoft', 'Azure', 'Natural'),\n ('Microsoft', 'Azure', 'Natural', 'Language'),\n ('Azure', 'Natural', 'Language', 'Processing')]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find 4-grams\n",
    "\n",
    "four_grams = nltk.ngrams(tokens, 4)\n",
    "list(four_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the War of the Worlds\n",
    "\n",
    "with open(\"wotw.txt\") as f:\n",
    "    wotw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\maxen\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\maxen\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "# code for preprocessing \n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def lower_tokens(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def remove_punctuation_tokens(tokens):\n",
    "    punct_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    return [a for a,b in zip(tokens, [punct_regex.sub('', token) for token in tokens]) if b != '']\n",
    "\n",
    "def get_cleaned_tokens(tokens):\n",
    "    return remove_punctuation_tokens(lower_tokens(tokens))\n",
    "\n",
    "def remove_stopword_tokens(tokens):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "def remove_punctuation_in_tokens(tokens):\n",
    "    punct_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    return [punct_regex.sub('', token) for token in tokens]\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    return remove_stopword_tokens(remove_punctuation_in_tokens(get_cleaned_tokens(nltk.word_tokenize(sentence))))\n",
    "\n",
    "def preprocess_tokens(tokens):\n",
    "    return remove_stopword_tokens(remove_punctuation_in_tokens(get_cleaned_tokens(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess \n",
    "\n",
    "tokens = preprocess_sentence(wotw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "29662"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bigram measures, determines if a bigram is meaningful \n",
    "# using a number of statistical measures\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# this class looks through all bigrams as potential collocations\n",
    "\n",
    "finder = nltk.BigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "#  all bigrams with count < 3 are not significant\n",
    "finder.apply_freq_filter(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('red', 'weed'),\n ('black', 'smoke'),\n ('could', 'see'),\n ('ulla', 'ulla'),\n ('came', 'upon'),\n ('far', 'away'),\n ('along', 'road'),\n ('another', 'moment'),\n ('ca', 'nt'),\n ('pine', 'trees'),\n ('one', 'another'),\n ('first', 'time'),\n ('hundred', 'yards'),\n ('one', 'two'),\n ('edge', 'pit')]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the top 15 bigrams based on raw frequency\n",
    "\n",
    "matches = finder.nbest(bigram_measures.raw_freq, 15)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}