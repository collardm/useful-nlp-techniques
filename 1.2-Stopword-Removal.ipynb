{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the needed imports\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some helper functions\n",
    "\n",
    "def lower_tokens(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def remove_punctuation_tokens(tokens):\n",
    "    punct_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    return [a for a,b in zip(tokens, [punct_regex.sub('', token) for token in tokens]) if b != '']\n",
    "\n",
    "def get_cleaned_tokens(tokens):\n",
    "    return remove_punctuation_tokens(lower_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text to tokenize\n",
    "\n",
    "text = \"The early bird gets the worm. \" + \\\n",
    "       \"Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb. \" + \\\n",
    "       \"The Earth is 92,960,000mi from the Sun. \" + \\\n",
    "       \"In Mr. Smith's words, 'This book is great!' \" + \\\n",
    "       \"The cost is $19.99\\non sale until the end of the year. \" + \\\n",
    "       \"Michio Kaku, Ph.D. \"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the necessary punkt items so we can tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and sample the 4th sentence\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(text)\n",
    "sample_sentence = sent_tokens[3]\n",
    "sample_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's word tokenize that sentence\n",
    "\n",
    "tokens = nltk.word_tokenize(sample_sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now install the stopwords items into nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check out all the stopwords in english\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the languages supported for stopwords\n",
    "\n",
    "!ls /home/azureuser/nltk_data/corpora/stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the stopwords from the list of tokens\n",
    "\n",
    "[token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that didn't work too well because of case, so let's lower case our tokens\n",
    "\n",
    "tokens = get_cleaned_tokens(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we'll see the stopwords removed\n",
    "\n",
    "[token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so let's make a function to remove stopwords\n",
    "\n",
    "def remove_stopword_tokens(tokens):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and give it a try\n",
    "\n",
    "remove_stopword_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we have a 's token, so let's remove punctuation in any tokens\n",
    "\n",
    "def remove_punctuation_in_tokens(tokens):\n",
    "    punct_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    return [punct_regex.sub('', token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_punct_removed = remove_punctuation_in_tokens(tokens)\n",
    "tokens_punct_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then remove stopwords\n",
    "\n",
    "remove_stopword_tokens(tokens_punct_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so let's pull this all together\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    return remove_stopword_tokens(remove_punctuation_in_tokens(get_cleaned_tokens(nltk.word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_sentence(sample_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
